{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["fR0ZgaLEBwua"],"authorship_tag":"ABX9TyMmJ4vtrGetfeLILnYppta4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"X21EnJv2_cA8","executionInfo":{"status":"ok","timestamp":1734865595203,"user_tz":-300,"elapsed":10187,"user":{"displayName":"Abbas Mirza","userId":"04098176250439025995"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F"]},{"cell_type":"markdown","source":["#Option 1 - Creating nn Modules:"],"metadata":{"id":"fR0ZgaLEBwua"}},{"cell_type":"code","source":["class NN(nn.Module):\n","\n","  def __init__(self, input_size, hidden_size):\n","\n","    super(NN, self).__init__()\n","\n","    self.linear1 = nn.Linear(input_size, hidden_size)\n","    self.relu = nn.ReLU()\n","    #Other activation functions:\n","    #nn.Sigmoid\n","    #nn.Softmax\n","    #nn.TanH\n","    #nn.LeakyReLU\n","    self.linear2 = nn.Linear(hidden_size, 1)\n","    self.sigmoid = nn.Sigmoid()\n","\n","  def forward(self, x):\n","    out = self.linear1(x)\n","    out = self.relu(out)\n","    out = self.linear2(out)\n","    out = self.sigmoid(out)\n","\n","    return out"],"metadata":{"id":"RYaudmq-B5bN","executionInfo":{"status":"ok","timestamp":1734865597103,"user_tz":-300,"elapsed":324,"user":{"displayName":"Abbas Mirza","userId":"04098176250439025995"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["#Option 2 - Use Activation Functions directly in forward pass"],"metadata":{"id":"IVfgthoyCfDc"}},{"cell_type":"code","source":["class NN(nn.Module):\n","\n","  def __init__(self, input_size, hidden_size):\n","\n","    super(NN, self).__init__()\n","\n","    self.linear1 = nn.Linear(input_size, hidden_size)\n","    self.linear2 = nn.Linear(hidden_size, 1)\n","\n","  def forward():\n","    out = torch.relu(self.linear1(x))\n","    out = torch.sigmoid(self.linear2(out))\n","    #Other activation functions:\n","    #torch.softmax\n","    #torch.tanh\n","    #F.reLu\n","    #F.leaky_relu(), only available in nn.Functional\n","\n","    return out"],"metadata":{"id":"ly2zUDNdCj0r","executionInfo":{"status":"ok","timestamp":1734865600078,"user_tz":-300,"elapsed":317,"user":{"displayName":"Abbas Mirza","userId":"04098176250439025995"}}},"execution_count":3,"outputs":[]}]}